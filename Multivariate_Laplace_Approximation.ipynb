{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b675842-5d58-42b3-b93f-08c92abb953a",
   "metadata": {},
   "source": [
    "# Error Estimation using the Laplace Approximation\n",
    "In one of the previous lectures, we introduced the Laplace approximation, which assumes that the posterior distribution can be described as a Gaussian distribution.  Here we show how to deal with multi-variate Gaussians\n",
    "As a reminder/refresher:\n",
    "$$p_{\\boldsymbol\\beta}(\\beta_1,\\ldots,\\beta_k) = \\frac{\\exp\\left(-\\frac 1 2 \\left({\\boldsymbol\\beta} - {\\boldsymbol\\mu}\\right)^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}\\left({\\boldsymbol\\beta}-{\\boldsymbol\\mu}\\right)\\right)}{\\sqrt{(2\\pi)^k |\\boldsymbol\\Sigma|}}$$\n",
    "where ${\\boldsymbol\\beta}$ is a real ''k''-dimensional column vector and $|\\boldsymbol\\Sigma|\\equiv \\det\\boldsymbol\\Sigma$ is the determinant of $\\boldsymbol\\Sigma$, also known as the generalized variance or covariance matrix. The equation above reduces to that of the univariate normal distribution if $\\boldsymbol\\Sigma$ is a $1 \\times 1$ matrix (i.e., a single real number).\n",
    "\n",
    "Similar to the one-dimensional case, we obtain the covariance matrix by taking the second derivative of the negative logarithm of $p_{\\boldsymbol\\beta}(\\beta_1,\\ldots,\\beta_k)$.  This is equivalent to the matrix inverse of the Hessian of $\\mathcal{X}^2$ with respect to the parameters $\\boldsymbol\\beta$.\n",
    "Or:\n",
    "$${\\boldsymbol\\Sigma} = 2\\begin{bmatrix}\n",
    "  \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_1^2} & \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_1\\,\\partial \\beta_2} & \\cdots & \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_1\\,\\partial \\beta_n} \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 \\mathcal{X}^2(\\mathbf x)}{\\partial \\beta_2\\,\\partial \\beta_1} & \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_2^2} & \\cdots & \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_2\\,\\partial \\beta_n} \\\\[2.2ex]\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\[2.2ex]\n",
    "  \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_n\\,\\partial \\beta_1} & \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_n\\,\\partial \\beta_2} & \\cdots & \\dfrac{\\partial^2 \\mathcal{X}^2(\\boldsymbol\\beta)}{\\partial \\beta_n^2}\n",
    "\\end{bmatrix}^{-1}$$\n",
    "As a reminder:\n",
    "$$\\mathcal{X}^2(\\boldsymbol\\beta) = \\sum_{i=1}^{N}\\frac{(y_{i}-f(x_{i},\\boldsymbol\\beta))^2}{\\sigma_{i}^2}$$\n",
    "$$p(\\boldsymbol\\beta) = e^{-\\mathcal{X}^{2}/2}$$\n",
    "where $f(x,\\boldsymbol\\beta)$ is the model with parameters $\\boldsymbol\\beta$, and $\\{y_{i},x_{i}\\}$ are the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49381c93-134d-46f4-a882-6aade4154a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
